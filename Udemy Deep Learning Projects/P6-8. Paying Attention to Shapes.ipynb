{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Udemy 6-8. Paying Attention to Shapes.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNkYiV5FoNzSJEpdrVeuP5c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Udemy 6-8. Paying Attention to Shapes**\n"],"metadata":{"id":"MoSqr4aGUpQP"}},{"cell_type":"markdown","source":["\n","Class Notes:\n","\n","---\n","\n","\n","\n","Simple Recurrent Unit or Elman Unit\n","\n","$\n","h_t = σ(W_{xh}^T. x_t + W_{hh}^T. h_{t-1} + b_h)\n","$\n","\n","Prediction will be\n","\n","$\n","ŷ_t = σ(W_o^T .h_t + b_o)\n","$\n","\n","---\n","Option 1:\n","\n","if `batch_first == True`\n","\n","Input must be N x T x D\n","\n","Option 2:\n","\n","if `batch_first == False`\n","\n","Input must be T x N x D\n","\n","---"],"metadata":{"id":"zXbX3fI6ZXhy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I58NIso9Uiru"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np"]},{"cell_type":"code","source":["# Things you should automatically know and have memorized\n","# N: number of samples\n","# T: length of the sequence\n","# D: number of input features\n","# M: number of hidden units\n","# K: number of output units"],"metadata":{"id":"S3NVrNZ3UwqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make some Data\n","N = 1\n","T = 10\n","D = 3\n","M = 5\n","K = 2\n","X = np.random.randn(N, T, D) # because batch_first=True, if batch_first=False then (T, N, D)"],"metadata":{"id":"fcr_IqPjU0XH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PART 1 - RNN using Torch\n","# Make an RNN\n","class SimpleRNN(nn.Module):\n","    def __init__(self, n_inputs, n_hidden, n_output):\n","        super().__init__()\n","        self.D = n_inputs\n","        self.M = n_hidden\n","        self.K = n_output\n","        self.rnn = nn.RNN(input_size=self.D,\n","                          hidden_size=self.M,\n","                          nonlinearity='tanh',\n","                          batch_first=True,)\n","        self.fc = nn.Linear(self.M, self.K)\n","        \n","    def forward(self, X):\n","        # initialize h0\n","        h0 = torch.zeros(1, X.size(0), self.M) # L x N x M\n","        \n","        # get RNN unit output\n","        out, _ = self.rnn(X, h0) # X: N x T x D | out: N x T x M\n","        \n","        # passing all hidden state (h_0 ... h_T) through dense layer\n","        out = self.fc(out) # in: N x T x M | out: N x T x K\n","        return out"],"metadata":{"id":"Z2jKrwwsU4ou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instantiate the model\n","model = SimpleRNN(n_inputs=D, n_hidden=M, n_output=K)"],"metadata":{"id":"HOg1T-YOU-J4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the output\n","inputs = torch.from_numpy(X.astype(np.float32))\n","outputs = model(inputs)\n","print(outputs)\n","print(outputs.shape) # out: N x T x K"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4SL4zFiVAGh","executionInfo":{"status":"ok","timestamp":1644491444764,"user_tz":-210,"elapsed":466,"user":{"displayName":"Mohammadali Rastin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08065540036492028655"}},"outputId":"f3cfb8c1-9a27-4f8d-9c29-f470bf48f5a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.2375,  0.1605],\n","         [-0.3742, -0.2028],\n","         [-0.7712,  0.0903],\n","         [-0.4133,  0.0852],\n","         [-0.3892,  0.7124],\n","         [-0.0583, -0.2509],\n","         [-0.8777,  0.7747],\n","         [-0.0092, -0.0253],\n","         [-0.3508,  0.5767],\n","         [-0.0639, -0.1212]]], grad_fn=<AddBackward0>)\n","torch.Size([1, 10, 2])\n"]}]},{"cell_type":"code","source":["# Save for later\n","Yhat_torch = outputs.detach().numpy()"],"metadata":{"id":"nqE45GcIVDA5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the RNN layer parameters\n","W_xh, W_hh, b_xh, b_hh = model.rnn.parameters()\n","print(\"W_xh.shape\", W_xh.shape)\n","print(W_xh)\n","\n","W_xh = W_xh.data.numpy()\n","W_hh = W_hh.data.numpy()\n","b_xh = b_xh.data.numpy()\n","b_hh = b_hh.data.numpy()\n","\n","print(W_xh.shape, b_xh.shape, W_hh.shape, b_hh.shape) # MxD, Dx1, MxM, Mx1 "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S01muZxRVFq5","executionInfo":{"status":"ok","timestamp":1644491501209,"user_tz":-210,"elapsed":482,"user":{"displayName":"Mohammadali Rastin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08065540036492028655"}},"outputId":"c4a43fa5-68c1-445f-ba16-8fa8d9816d67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["W_xh.shape torch.Size([5, 3])\n","Parameter containing:\n","tensor([[-0.2946, -0.0276, -0.1723],\n","        [-0.4416, -0.4401, -0.4396],\n","        [ 0.2607,  0.2543, -0.3633],\n","        [ 0.3621,  0.3392,  0.1759],\n","        [-0.2745, -0.3040, -0.4026]], requires_grad=True)\n","(5, 3) (5,) (5, 5) (5,)\n"]}]},{"cell_type":"code","source":["# get the FC layer parameters\n","W_o, b_o = model.fc.parameters()\n","\n","W_o = W_o.data.numpy()\n","b_o = b_o.data.numpy()\n","\n","print(W_o.shape, b_o.shape) # KxM, Kx1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_cvAXZtVQLz","executionInfo":{"status":"ok","timestamp":1644491517811,"user_tz":-210,"elapsed":3,"user":{"displayName":"Mohammadali Rastin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08065540036492028655"}},"outputId":"611c03ff-e5a8-4599-dbe3-577e3d3d14fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(2, 5) (2,)\n"]}]},{"cell_type":"code","source":["# PART 2 - RNN using numpy\n","# Simplified because N is considered to be 1\n","# See if we can replicate the output\n","h_last = np.zeros(M) # initial hidden state\n","x = X[0]\n","print(x.shape)\n","yhats = np.zeros((T, K),) # where we store the outputs\n","\n","for t in range(T):\n","    h = np.tanh(x[t].dot(W_xh.T) + b_xh + h_last.dot(W_hh.T) + b_hh)\n","    y = h.dot(W_o.T) + b_o # We only care about this value on last iteration\n","    yhats[t] = y\n","    \n","    h_last = h # Don't forget to assign h to h_last\n","\n","print(yhats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Wq-p493VWxu","executionInfo":{"status":"ok","timestamp":1644491537534,"user_tz":-210,"elapsed":437,"user":{"displayName":"Mohammadali Rastin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08065540036492028655"}},"outputId":"a89a5817-3cdb-4b70-e9e7-757ecad1de9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(10, 3)\n","[[-0.23747324  0.16046794]\n"," [-0.37419039 -0.20281294]\n"," [-0.77121085  0.09033243]\n"," [-0.41333817  0.08524129]\n"," [-0.38919497  0.71243846]\n"," [-0.05832134 -0.25088813]\n"," [-0.87765071  0.7747328 ]\n"," [-0.0092352  -0.02526761]\n"," [-0.35084117  0.57665854]\n"," [-0.06385781 -0.12120582]]\n"]}]},{"cell_type":"code","source":["# Check\n","print(np.allclose(yhats, Yhat_torch)) # Both nn.RNN and our formula respond the same"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"47RFBfDsViXN","executionInfo":{"status":"ok","timestamp":1644491647224,"user_tz":-210,"elapsed":423,"user":{"displayName":"Mohammadali Rastin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08065540036492028655"}},"outputId":"585667e5-275b-41ea-ab42-ca49d323d799"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["# Excersize: Calculate the output for multiple samples at once (N > 1)\n","# Response: \"Udemy 6-8. Paying Attention to Shapes - Appendix A\""],"metadata":{"id":"7qdEIoo3VzPX"},"execution_count":null,"outputs":[]}]}